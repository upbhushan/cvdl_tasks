{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\nprint('hello')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:50.485258Z","iopub.execute_input":"2025-04-02T08:49:50.485593Z","iopub.status.idle":"2025-04-02T08:49:51.035894Z","shell.execute_reply.started":"2025-04-02T08:49:50.485572Z","shell.execute_reply":"2025-04-02T08:49:51.034805Z"}},"outputs":[{"name":"stdout","text":"hello\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!pip install torch torchvision albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:51.037232Z","iopub.execute_input":"2025-04-02T08:49:51.037577Z","iopub.status.idle":"2025-04-02T08:49:55.010607Z","shell.execute_reply.started":"2025-04-02T08:49:51.037542Z","shell.execute_reply":"2025-04-02T08:49:55.008606Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\nRequirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\nRequirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.29.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# Load pre-trained Faster R-CNN model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Modify the classifier for custom dataset\nnum_classes = 4  # Example: 3 classes + 1 background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.013113Z","iopub.execute_input":"2025-04-02T08:49:55.013394Z","iopub.status.idle":"2025-04-02T08:49:55.790370Z","shell.execute_reply.started":"2025-04-02T08:49:55.013367Z","shell.execute_reply":"2025-04-02T08:49:55.789344Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset\nimport xml.etree.ElementTree as ET\nfrom torch.utils.data import DataLoader\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.791672Z","iopub.execute_input":"2025-04-02T08:49:55.791981Z","iopub.status.idle":"2025-04-02T08:49:55.796719Z","shell.execute_reply.started":"2025-04-02T08:49:55.791958Z","shell.execute_reply":"2025-04-02T08:49:55.795727Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torchvision.transforms as T\n\nclass CustomDataset(Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms if transforms else T.ToTensor()  # Convert to tensor by default\n        self.imgs = sorted(os.listdir(os.path.join(root, \"images\")))\n        self.annotations = sorted(os.listdir(os.path.join(root, \"annotations\")))\n\n    def load_annotations(self, ann_path):\n        tree = ET.parse(ann_path)\n        root = tree.getroot()\n\n        boxes = []\n        labels = []\n        label_map = {\"with_mask\": 1, \"without_mask\": 2, \"mask_weared_incorrect\": 3}  # Modify as needed\n\n        for obj in root.findall(\"object\"):\n            name = obj.find(\"name\").text\n            labels.append(label_map[name])  \n\n            bndbox = obj.find(\"bndbox\")\n            xmin = int(bndbox.find(\"xmin\").text)\n            ymin = int(bndbox.find(\"ymin\").text)\n            xmax = int(bndbox.find(\"xmax\").text)\n            ymax = int(bndbox.find(\"ymax\").text)\n\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        return torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n        annotation_path = os.path.join(self.root, \"annotations\", self.annotations[idx])\n\n        image = Image.open(img_path).convert(\"RGB\")\n        boxes, labels = self.load_annotations(annotation_path)\n\n        target = {\"boxes\": boxes, \"labels\": labels}\n\n        # Convert image to tensor\n        image = self.transforms(image)\n\n        return image, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.797692Z","iopub.execute_input":"2025-04-02T08:49:55.797967Z","iopub.status.idle":"2025-04-02T08:49:55.817941Z","shell.execute_reply.started":"2025-04-02T08:49:55.797936Z","shell.execute_reply":"2025-04-02T08:49:55.816862Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((300, 400)),  # Resize all images to 300x400\n    transforms.ToTensor(),  # Convert to tensor\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.819018Z","iopub.execute_input":"2025-04-02T08:49:55.819245Z","iopub.status.idle":"2025-04-02T08:49:55.838253Z","shell.execute_reply.started":"2025-04-02T08:49:55.819223Z","shell.execute_reply":"2025-04-02T08:49:55.837337Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Apply the transformation when loading the dataset\ndataset = CustomDataset(\"/kaggle/input/face-mask-detection\", transforms=transform)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.839269Z","iopub.execute_input":"2025-04-02T08:49:55.839614Z","iopub.status.idle":"2025-04-02T08:49:55.857307Z","shell.execute_reply.started":"2025-04-02T08:49:55.839584Z","shell.execute_reply":"2025-04-02T08:49:55.856336Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"dataset = CustomDataset(\"/kaggle/input/face-mask-detection\")\nimage, target = dataset[0]\n\nprint(f\"Image Size: {image.size}\")\nprint(f\"Target: {target}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.860168Z","iopub.execute_input":"2025-04-02T08:49:55.860414Z","iopub.status.idle":"2025-04-02T08:49:55.888669Z","shell.execute_reply.started":"2025-04-02T08:49:55.860394Z","shell.execute_reply":"2025-04-02T08:49:55.887820Z"}},"outputs":[{"name":"stdout","text":"Image Size: <built-in method size of Tensor object at 0x7ca8a5571210>\nTarget: {'boxes': tensor([[ 79., 105., 109., 142.],\n        [185., 100., 226., 144.],\n        [325.,  90., 360., 141.]]), 'labels': tensor([2, 1, 2])}\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load full dataset\ndataset = CustomDataset(\"/kaggle/input/face-mask-detection\")\n\n# Generate train-test split indices\nindices = list(range(len(dataset)))\ntrain_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n\n# Create subsets\ntrain_dataset = torch.utils.data.Subset(dataset, train_indices)\ntest_dataset = torch.utils.data.Subset(dataset, test_indices)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.890162Z","iopub.execute_input":"2025-04-02T08:49:55.890457Z","iopub.status.idle":"2025-04-02T08:49:55.898950Z","shell.execute_reply.started":"2025-04-02T08:49:55.890434Z","shell.execute_reply":"2025-04-02T08:49:55.898065Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.899871Z","iopub.execute_input":"2025-04-02T08:49:55.900157Z","iopub.status.idle":"2025-04-02T08:49:55.912613Z","shell.execute_reply.started":"2025-04-02T08:49:55.900129Z","shell.execute_reply":"2025-04-02T08:49:55.911723Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.913408Z","iopub.execute_input":"2025-04-02T08:49:55.913683Z","iopub.status.idle":"2025-04-02T08:49:55.987702Z","shell.execute_reply.started":"2025-04-02T08:49:55.913663Z","shell.execute_reply":"2025-04-02T08:49:55.986926Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# Define optimizer\nlearning_rate = 0.005  # Try different values (e.g., 0.001, 0.01)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.988541Z","iopub.execute_input":"2025-04-02T08:49:55.988772Z","iopub.status.idle":"2025-04-02T08:49:55.992812Z","shell.execute_reply.started":"2025-04-02T08:49:55.988753Z","shell.execute_reply":"2025-04-02T08:49:55.991907Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:55.993739Z","iopub.execute_input":"2025-04-02T08:49:55.994014Z","iopub.status.idle":"2025-04-02T08:49:56.009502Z","shell.execute_reply.started":"2025-04-02T08:49:55.993993Z","shell.execute_reply":"2025-04-02T08:49:56.008749Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Training loop\nnum_epochs = 5  # Try different values\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]  # Now img is a tensor\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:56.010508Z","iopub.execute_input":"2025-04-02T08:49:56.010826Z","iopub.status.idle":"2025-04-02T08:49:56.024171Z","shell.execute_reply.started":"2025-04-02T08:49:56.010797Z","shell.execute_reply":"2025-04-02T08:49:56.023357Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"if we try for 0.001 learning rate and for 7 epochs than result is look like","metadata":{}},{"cell_type":"code","source":"# Define optimizer\nlearning_rate = 0.001  # Try different values (e.g., 0.001, 0.01)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n\n# Training loop\nnum_epochs = 7  # Try different values\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]  # Now img is a tensor\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:49:56.024938Z","iopub.execute_input":"2025-04-02T08:49:56.025194Z","iopub.status.idle":"2025-04-02T09:05:53.177997Z","shell.execute_reply.started":"2025-04-02T08:49:56.025173Z","shell.execute_reply":"2025-04-02T09:05:53.176957Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/7], Loss: 90.2359\nEpoch [2/7], Loss: 56.8272\nEpoch [3/7], Loss: 48.6230\nEpoch [4/7], Loss: 44.7022\nEpoch [5/7], Loss: 40.4689\nEpoch [6/7], Loss: 37.1928\nEpoch [7/7], Loss: 35.0686\n","output_type":"stream"}],"execution_count":42}]}